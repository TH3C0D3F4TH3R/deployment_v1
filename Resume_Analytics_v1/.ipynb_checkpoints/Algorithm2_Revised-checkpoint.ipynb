{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\spacy\\util.py:275: UserWarning: [W031] Model 'en_training' (0.0.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "from data_parser import resumeparse,base_path\n",
    "from file_reader import read_file\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import pandas as pd, numpy as np\n",
    "import requests\n",
    "import nltk\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import itertools\n",
    "import collections\n",
    "from bs4 import BeautifulSoup\n",
    "from rake_nltk import Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# URL = 'http://www.jobserve.com/JXTG1'\n",
    "# URL = 'http://www.jobserve.com/JDZxf'\n",
    "URL = 'http://www.jobserve.com/JXTHN'\n",
    "# URL = 'http://www.jobserve.com/JXTHJ'\n",
    "page = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobtitle = soup.find(id='td_jobpositionnolink')\n",
    "jobtype = soup.find(id='td_job_type')\n",
    "jobpostedby = soup.find(id='td_posted_by')\n",
    "posteddate = soup.find(id='td_posted_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Senior Front End JavaScript Developer | Angular | Greenfield Project\n",
      "Permanent\n",
      "TheDriveGroup\n",
      "Friday, 25 December 2020\n"
     ]
    }
   ],
   "source": [
    "#Job Data\n",
    "Job_Title = jobtitle.text\n",
    "Job_Type = jobtype.text\n",
    "Posted_By = jobpostedby.text.split(': ')[1]\n",
    "Posted_Date = posteddate.text.split(': ')[1]\n",
    "print(Job_Title)\n",
    "print(Job_Type)\n",
    "print(Posted_By)\n",
    "print(Posted_Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Getting skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will give us all the job details\n",
    "Job_Details = soup.find('div', {'class':'md_skills'})\n",
    "Job_Attributes = Job_Details.findAll('li')\n",
    "Job_Data = []\n",
    "for elem in Job_Attributes:\n",
    "    elem1 = elem.text\n",
    "    Job_Data.append(elem1)\n",
    "\n",
    "job_text= ' '.join(elem for elem in Job_Data)\n",
    "\n",
    "# job_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10 javascript typescript html5',\n",
       " 'js graphql nodejs',\n",
       " 'net core',\n",
       " 'css3 react',\n",
       " 'azure ci',\n",
       " 'angular 8',\n",
       " 'kubernetes',\n",
       " 'docker',\n",
       " 'devops',\n",
       " 'cd',\n",
       " '9']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = Rake()\n",
    "r.extract_keywords_from_text(job_text)\n",
    "\n",
    "skill_list = r.get_ranked_phrases()\n",
    "# skill_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will create list of relevant words\n",
    "lookup = []\n",
    "for elem in Job_Data:\n",
    "    temp_list= elem.split(',')\n",
    "    for i in temp_list:\n",
    "        lookup.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A master lookup file\n",
    "lookup_master = []\n",
    "for elem in lookup:\n",
    "    temp_list= elem.split(' ')\n",
    "    for i in temp_list:\n",
    "        lookup_master.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing our skill file\n",
    "file = ''\n",
    "base_path = os.path.dirname(file)\n",
    "file = os.path.join(base_path,\"data_files/SKILLS_1.txt\")\n",
    "file = open(file, \"r\", encoding='utf-8')    \n",
    "skill_file = [line.strip().lower() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'',\n",
       " 'angular',\n",
       " 'azure',\n",
       " 'c',\n",
       " 'devops',\n",
       " 'docker',\n",
       " 'graphql',\n",
       " 'html5',\n",
       " 'javascript',\n",
       " 'nodejs',\n",
       " 'ps',\n",
       " 'r',\n",
       " 'react',\n",
       " 'rn',\n",
       " 'typescript'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stage 1 Match\n",
    "match=[]\n",
    "for skill in skill_file:\n",
    "    for job_skill in skill_list:\n",
    "        if len(job_skill.split(' '))==1:\n",
    "            if skill.upper() in job_skill.strip().upper():\n",
    "                match.append(skill)\n",
    "        else:\n",
    "            for s in job_skill.split(' '):\n",
    "#                 print(s)\n",
    "                if skill.upper() == s.strip().upper():\n",
    "                    match.append(skill)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FInal Match\n",
    "final = []\n",
    "for item in match:\n",
    "    for master_item in lookup_master:\n",
    "        if item.lower()==master_item.lower().strip():\n",
    "            final.append(item.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'', 'ANGULAR', 'GRAPHQL', 'JAVASCRIPT', 'NODEJS', 'TYPESCRIPT'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Getting skill count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create counter\n",
    "word_count = collections.Counter(lookup_master)\n",
    "WordCount = {k.upper():v for k,v in word_count.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GRAPHQL': 1,\n",
       " '': 1,\n",
       " 'ANGULAR': 1,\n",
       " 'TYPESCRIPT': 1,\n",
       " 'JAVASCRIPT': 1,\n",
       " 'NODEJS': 1}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict={}\n",
    "for item in list(set(final)):\n",
    "    key = item\n",
    "\n",
    "    value = WordCount[key]\n",
    "    temp_dict = {key:value}\n",
    "    word_dict.update(temp_dict)\n",
    "word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_data(job_link,resume_skill):\n",
    "    \n",
    "    '''This function will extract job related data along with the matched skills from resume. As per requirement, we will also be capturing the skills not present in the resume but mentioned in the Job posting.\n",
    "    The count of skills willbe ectracted too'''\n",
    "\n",
    "    import os\n",
    "    import re\n",
    "    import time\n",
    "    from urllib.request import urlopen\n",
    "    import json\n",
    "    from pandas.io.json import json_normalize\n",
    "    import pandas as pd, numpy as np\n",
    "    import requests\n",
    "    import nltk\n",
    "    import spacy\n",
    "    from spacy.matcher import Matcher\n",
    "    from spacy.matcher import PhraseMatcher\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    import itertools\n",
    "    import collections\n",
    "    from bs4 import BeautifulSoup\n",
    "    from rake_nltk import Rake\n",
    "    \n",
    "    # Extracting job posting\n",
    "    page = requests.get(job_link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    jobtitle = soup.find(id='td_jobpositionnolink')\n",
    "    jobtype = soup.find(id='td_job_type')\n",
    "    jobpostedby = soup.find(id='td_posted_by')\n",
    "    posteddate = soup.find(id='td_posted_date')\n",
    "    \n",
    "    #Job Data\n",
    "    Job_Title = jobtitle.text\n",
    "    Job_Type = jobtype.text\n",
    "    Posted_By = jobpostedby.text.split(': ')[1]\n",
    "    Posted_Date = posteddate.text.split(': ')[1]\n",
    "#     print(Job_Title)\n",
    "#     print(Job_Type)\n",
    "#     print(Posted_By)\n",
    "#     print(Posted_Date)\n",
    "\n",
    "    # This will give us all the job details\n",
    "    Job_Details = soup.find('div', {'class':'md_skills'})\n",
    "    Job_Attributes = Job_Details.findAll('li')\n",
    "    Job_Data = []\n",
    "    for elem in Job_Attributes:\n",
    "        elem1 = elem.text\n",
    "        Job_Data.append(elem1)\n",
    "\n",
    "    job_text= ' '.join(elem for elem in Job_Data)\n",
    "\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(job_text)\n",
    "\n",
    "    skill_list = r.get_ranked_phrases()\n",
    "    \n",
    "    lookup = []\n",
    "    for elem in Job_Data:\n",
    "        temp_list= elem.split(',')\n",
    "        for i in temp_list:\n",
    "            lookup.append(i)\n",
    "            \n",
    "    lookup_master = []\n",
    "    for elem in lookup:\n",
    "        temp_list= elem.split(' ')\n",
    "        for i in temp_list:\n",
    "            lookup_master.append(i)\n",
    "            \n",
    "    # Importing our skill file\n",
    "    file = ''\n",
    "    base_path = os.path.dirname(file)\n",
    "    file = os.path.join(base_path,\"data_files/SKILLS_1.txt\")\n",
    "    file = open(file, \"r\", encoding='utf-8')    \n",
    "    skill_file = [line.strip().lower() for line in file]   \n",
    "    \n",
    "    # Stage 1 Match\n",
    "    match=[]\n",
    "    for skill in skill_file:\n",
    "        for job_skill in skill_list:\n",
    "            if len(job_skill.split(' '))==1:\n",
    "                if skill.upper() in job_skill.strip().upper():\n",
    "                    match.append(skill)\n",
    "            else:\n",
    "                for s in job_skill.split(' '):\n",
    "    #                 print(s)\n",
    "                    if skill.upper() == s.strip().upper():\n",
    "                        match.append(skill)\n",
    "                \n",
    "    final = []\n",
    "    for item in match:\n",
    "        for master_item in lookup_master:\n",
    "            if item.lower()==master_item.lower().strip():\n",
    "                final.append(item.upper())\n",
    "                \n",
    "    final_skills = list(set(final))   \n",
    "    num_skills = len(final_skills)\n",
    "    # Create counter\n",
    "    word_count = collections.Counter(lookup_master)\n",
    "    WordCount = {k.upper():v for k,v in word_count.items()}\n",
    "    \n",
    "    word_dict={}\n",
    "    for item in list(set(final)):\n",
    "        key = item\n",
    "\n",
    "        value = WordCount[key]\n",
    "        temp_dict = {key:value}\n",
    "        word_dict.update(temp_dict)\n",
    "        \n",
    "    # Fetching skills not present in resume but mentioned in job posting\n",
    "    matched_skill_count=0\n",
    "    unmatched_skill_count=0\n",
    "    for skill in final_skills:\n",
    "        for res_skill in resume_skill:\n",
    "            if skill.upper()== res_skill.strip().upper():\n",
    "                matched_skill_count+=1\n",
    "            else:\n",
    "                unmatched_skill_count+=1\n",
    "        \n",
    "    output = {\n",
    "        \"job_title\" : Job_Title,\n",
    "        \"job_type\":Job_Type,\n",
    "        \"posted_by\": Posted_By,\n",
    "        \"posted_date\": Posted_Date,\n",
    "        \"job_skills\": final_skills,\n",
    "        \"job_skill_count\": word_dict,\n",
    "        \"Num_of_job_skills\": num_skills,\n",
    "        \"matched_skill_count\": matched_skill_count,\n",
    "        \"unmatched_skill_count\":unmatched_skill_count\n",
    "        }\n",
    "    \n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
